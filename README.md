# Predictive_Maintenance_LSTM
Many newest machines, engines and devices are today equipped with several sensors delivering constantly lots of valuable data. This data may often help to predict the possible Rest Useful Life (RUL) of the machine and thus prevent from unexpected and unwanted breakdown and lost of service. Big amount of acquired data and sometimes quite a complicated correlation between them and machine failure led to involve an implementation of machine learning to survey this status and raise the alarm early enough to schedule the necessary maintenance.
Implementation of classical machine learning classifiers like: Gaussian Naïve Bayes, Bernoulli Naïve Bayes, K Neighbors, Logistic Regression,
Gradient Boosting, Random Forest, XG Boost or Support Vector Classifier may bring quite a good result in many cases indicating increased probability of the failure under certain sensor values. These algorithms will however not catch the important change of sensor data in time which is crucial in predicting the status of the machine in the future. These algorithm do not have the "time memory" which would allow first to learn and then to decode time trends. 
A good candidate which has this feature is Long Short-Term Memory (LSTM). Without entering into details model LSTM adds an additional time "layer" to the previous classical models. This so called memory will be used in our case to learn and than to recognize trends in time series. This powerful advantage however requires even better data cleaning and preprocessing. Most of this preparation work is assuring the data consistency. To train a good performing model you need to check and annotate data for a certain period of time. For a repetitive events, a rule of thumb here is to follow the Nyquist rate in sampling process. It means if you like to recognize correctly the event which repeats every 1000 samples, than you need to annotate and deliver to training algorithm a data of at least 2001 samples. If the event is not repetitive the number of samples should be long enough to catch at least a few "events". In case of Predictive Maintenance this is one of the most difficult conditions, because you get a very sparse information about failing events. Most of samples, sometimes 99.99% are OK samples. Another importing challenge is choosing the correct "observation window". This sliding,  window should have size long enough to learn and decode the trend but not too big to make the model overfitted.
In my study I analyzed highly unbalanced telemetry dataset of machines. Proper cleaning dataset and implementing the LSTM improved the precision and recall from 80% when implementing classical models to 99% with LSTM.
